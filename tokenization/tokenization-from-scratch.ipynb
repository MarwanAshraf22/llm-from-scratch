{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "WBPA--4WrK3B",
      "metadata": {
        "id": "WBPA--4WrK3B"
      },
      "source": [
        "# Tokenization from Scratch\n",
        "\n",
        "This walkthrough builds a tiny, regex-based tokenizer step by step. We start by loading a raw text sample, experiment with different splitting heuristics, and then evolve the tokenizer to support vocabularies and special tokens. Feel free to run cells in order or jump into sections that interest you.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Inspect the Raw Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfd5120d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfd5120d",
        "outputId": "bfb301af-79c8-48bd-a8a0-45efe65bf37d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total number of characters: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "# Load a short story from the local filesystem\n",
        "with open('the-verdict.txt', \"r\", encoding='utf-8') as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "# Quick sanity checks: overall length and a peek at the first 100 characters\n",
        "print('total number of characters:', len(raw_text))\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff62d886",
      "metadata": {},
      "source": [
        "### 1.1 Basic Stats\n",
        "\n",
        "A quick double-check of the character count helps ensure the file loaded correctly. Feel free to delete or skip this cell once you're confident in the setup.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xaULruBLqVhV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaULruBLqVhV",
        "outputId": "48fa6ae5-ce68-47b5-a75c-753588052684"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20479\n"
          ]
        }
      ],
      "source": [
        "print(len(raw_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a3bc4c1",
      "metadata": {},
      "source": [
        "## 2. Experiment with Regex-Based Splitting\n",
        "\n",
        "Before touching the real text, it's useful to practice on a tiny example. The next few cells incrementally refine the regular expression so we understand which characters end up as individual tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc6ddb37",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc6ddb37",
        "outputId": "ba137f4e-9b73-464a-d8fa-e3c97f737983"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'this,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, world. this, is a test.\"\n",
        "# Start simple: split only on whitespace to see how punctuation clings to the words\n",
        "result = re.split(r'(\\s)', text)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecfda998",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecfda998",
        "outputId": "129e4b81-06b5-4789-95fa-7cdbf1f7de34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'this', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ],
      "source": [
        "# Add commas and periods to the delimiter set; note the empty strings caused by grouped matches\n",
        "result = re.split(r'([,.]|\\s)', text)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab6bf1fe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab6bf1fe",
        "outputId": "8690d5f5-e5dc-4f6f-d02b-0e61fa0c0d73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ' ', 'world', ' ', 'this', ' ', 'is', ' ', 'a', ' ', 'test']\n"
          ]
        }
      ],
      "source": [
        "# Filter out the empty strings introduced by the capturing groups\n",
        "result = [item for item in result if item.strip(',.')]\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7dc136c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7dc136c",
        "outputId": "811f6d61-2e76-4f1b-bfdc-c586c7b25a12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'world', '.', 'this', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ],
      "source": [
        "# Expand the delimiter list to include more punctuation as separate tokens\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item for item in result if item.strip()]\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ccc8dd4",
      "metadata": {},
      "source": [
        "## 3. Tokenize the Full Corpus\n",
        "\n",
        "With the expression behaving nicely on the toy example, we can apply it to the entire story. The first cell previews the first few tokens; the second lets us confirm how many tokens we produced.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nfYkox8Xo8X1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfYkox8Xo8X1",
        "outputId": "c23ce076-c22f-4605-c153-c471e6e35dd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
          ]
        }
      ],
      "source": [
        "# Apply the regex to the full story and drop pure-whitespace fragments\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item for item in preprocessed if item.strip()]\n",
        "\n",
        "print(preprocessed[:30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CiUXStEzqKxQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiUXStEzqKxQ",
        "outputId": "d811eb9c-8fab-4717-924f-3c9f8995bcdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4690\n"
          ]
        }
      ],
      "source": [
        "# Total number of tokens extracted with the current regex\n",
        "print(len(preprocessed))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8T3E8WHwv7ao",
      "metadata": {
        "id": "8T3E8WHwv7ao"
      },
      "source": [
        "## 4. Build a Vocabulary\n",
        "\n",
        "We can now gather the unique tokens, inspect the vocabulary size, and create lookup tables that map between strings and integers. These tables are the backbone of our first tokenizer implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0XkHTWSRqSj9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XkHTWSRqSj9",
        "outputId": "857bb914-1a4c-4c7a-ac58-cf66b37884dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1130\n"
          ]
        }
      ],
      "source": [
        "# Collect unique tokens and inspect the vocabulary size\n",
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mrpre1dWxRBd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mrpre1dWxRBd",
        "outputId": "b0182b76-1a14-486c-d98c-24d6f3c275a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n",
            "('Claude', 26)\n",
            "('Come', 27)\n",
            "('Croft', 28)\n",
            "('Destroyed', 29)\n",
            "('Devonshire', 30)\n",
            "('Don', 31)\n",
            "('Dubarry', 32)\n",
            "('Emperors', 33)\n",
            "('Florence', 34)\n",
            "('For', 35)\n",
            "('Gallery', 36)\n",
            "('Gideon', 37)\n",
            "('Gisburn', 38)\n",
            "('Gisburns', 39)\n",
            "('Grafton', 40)\n",
            "('Greek', 41)\n",
            "('Grindle', 42)\n",
            "('Grindles', 43)\n",
            "('HAD', 44)\n",
            "('Had', 45)\n",
            "('Hang', 46)\n",
            "('Has', 47)\n",
            "('He', 48)\n",
            "('Her', 49)\n",
            "('Hermia', 50)\n"
          ]
        }
      ],
      "source": [
        "# Map token strings to integer ids (string -> id)\n",
        "encoder = {token: integer for integer, token in enumerate(all_words)}\n",
        "for i, item in enumerate(encoder.items()):\n",
        "    print(item)\n",
        "    if i >= 50:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TsKWTSG81dN2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TsKWTSG81dN2",
        "outputId": "9934b032-568d-441d-e4e8-1407d14ee061"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0, '!')\n",
            "(1, '\"')\n",
            "(2, \"'\")\n",
            "(3, '(')\n",
            "(4, ')')\n",
            "(5, ',')\n",
            "(6, '--')\n",
            "(7, '.')\n",
            "(8, ':')\n",
            "(9, ';')\n",
            "(10, '?')\n",
            "(11, 'A')\n",
            "(12, 'Ah')\n",
            "(13, 'Among')\n",
            "(14, 'And')\n",
            "(15, 'Are')\n",
            "(16, 'Arrt')\n",
            "(17, 'As')\n",
            "(18, 'At')\n",
            "(19, 'Be')\n",
            "(20, 'Begin')\n",
            "(21, 'Burlington')\n",
            "(22, 'But')\n",
            "(23, 'By')\n",
            "(24, 'Carlo')\n",
            "(25, 'Chicago')\n",
            "(26, 'Claude')\n",
            "(27, 'Come')\n",
            "(28, 'Croft')\n",
            "(29, 'Destroyed')\n",
            "(30, 'Devonshire')\n",
            "(31, 'Don')\n",
            "(32, 'Dubarry')\n",
            "(33, 'Emperors')\n",
            "(34, 'Florence')\n",
            "(35, 'For')\n",
            "(36, 'Gallery')\n",
            "(37, 'Gideon')\n",
            "(38, 'Gisburn')\n",
            "(39, 'Gisburns')\n",
            "(40, 'Grafton')\n",
            "(41, 'Greek')\n",
            "(42, 'Grindle')\n",
            "(43, 'Grindles')\n",
            "(44, 'HAD')\n",
            "(45, 'Had')\n",
            "(46, 'Hang')\n",
            "(47, 'Has')\n",
            "(48, 'He')\n",
            "(49, 'Her')\n",
            "(50, 'Hermia')\n"
          ]
        }
      ],
      "source": [
        "# Map integer ids back to their token strings (id -> string)\n",
        "decoder = {idx: token for idx, token in enumerate(all_words)}\n",
        "for i, item in enumerate(decoder.items()):\n",
        "    print(item)\n",
        "    if i >= 50:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5ea6577",
      "metadata": {},
      "source": [
        "## 5. Implement `SimpleTokenizerV1`\n",
        "\n",
        "The first tokenizer assumes the input text only contains tokens seen during preprocessing. The encode path mirrors the earlier regex pipeline, while decode stitches tokens back into a readable string.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BWudx-mIyWv1",
      "metadata": {
        "id": "BWudx-mIyWv1"
      },
      "outputs": [],
      "source": [
        "class SimpleTokenizerV1:\n",
        "    \"\"\"A minimal tokenizer that works only for in-vocabulary tokens.\"\"\"\n",
        "\n",
        "    def __init__(self, encoder, decoder):\n",
        "        self.str_to_int = encoder\n",
        "        self.int_to_str = decoder\n",
        "\n",
        "    def encode(self, text: str) -> list[int]:\n",
        "        \"\"\"Split text with the earlier regex and map each token to an id.\"\"\"\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids: list[int]) -> str:\n",
        "        \"\"\"Convert ids back to tokens and clean up spacing around punctuation.\"\"\"\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-ZHJSE97ogyW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-ZHJSE97ogyW",
        "outputId": "1edbbcb5-a329-47dd-d1d5-af00079cb5ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[56, 2, 850, 988, 602, 533, 746, 5, 1126, 596]\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the tokenizer and try encoding a short phrase\n",
        "tokenizer = SimpleTokenizerV1(encoder, decoder)\n",
        "\n",
        "text = \"It's the last he painted, you know\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "F3tBA4JazAQB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3tBA4JazAQB",
        "outputId": "637b4134-916e-479c-823b-03dbd33659f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It' s the last he painted, you know\n"
          ]
        }
      ],
      "source": [
        "# Decoding reverses the process, though notice the spacing issue around the apostrophe\n",
        "text = tokenizer.decode(ids)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M9O3gLU-le0G",
      "metadata": {
        "id": "M9O3gLU-le0G"
      },
      "source": [
        "## 6. Handle Special Tokens and Unknown Words\n",
        "\n",
        "Real-world tokenizers reserve ids for things like sequence termination and unknown words. The next few cells extend the vocabulary and introduce `<|unk|>` handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H2CJVYs-8erA",
      "metadata": {
        "id": "H2CJVYs-8erA"
      },
      "outputs": [],
      "source": [
        "# Extend the vocabulary with special tokens commonly used in language models\n",
        "all_tokens = sorted(set(preprocessed))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab = {token: integer for integer, token in enumerate(all_tokens)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3TUR7wB4nKGr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TUR7wB4nKGr",
        "outputId": "142d8c10-ea98-45c9-bf37-c15160d5c7f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1132"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Confirm the new vocabulary size after adding special tokens\n",
        "len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oDMSWCQ5oAZ-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDMSWCQ5oAZ-",
        "outputId": "cc62ef4f-5d8f-44fa-c393-fa93d52ddb67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "younger\n",
            "your\n",
            "yourself\n",
            "<|endoftext|\n",
            "<|unk|>\n"
          ]
        }
      ],
      "source": [
        "# Peek at the tail end of the vocabulary to confirm special tokens are present\n",
        "for token, idx in list(vocab.items())[-5:]:\n",
        "    print(f\"{token!r} -> {idx}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wxPrUtPc-XHg",
      "metadata": {
        "id": "wxPrUtPc-XHg"
      },
      "outputs": [],
      "source": [
        "# Rebuild encoder/decoder to include the special tokens\n",
        "encoder = {token: idx for idx, token in enumerate(all_tokens)}\n",
        "decoder = {idx: token for idx, token in enumerate(all_tokens)}\n",
        "\n",
        "\n",
        "class SimpleTokenizerV2:\n",
        "    \"\"\"Adds `<|unk|>` support by mapping unseen tokens to a fallback id.\"\"\"\n",
        "\n",
        "    def __init__(self, encoder, decoder):\n",
        "        self.str_to_int = encoder\n",
        "        self.int_to_str = decoder\n",
        "\n",
        "    def encode(self, text: str) -> list[int]:\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids: list[int]) -> str:\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1111wKzb-Bbo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1111wKzb-Bbo",
        "outputId": "927eab43-0126-4dbf-84e8-ac03c88fbbd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 1131]\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the improved tokenizer and encode a string with an unknown token\n",
        "tokenizer = SimpleTokenizerV2(encoder, decoder)\n",
        "\n",
        "text = \"It's the last he painted, you know ss\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A2zhnViN-DzA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2zhnViN-DzA",
        "outputId": "943cdd0f-20b3-4d9f-8dff-16fc012174f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It' s the last he painted, you know <|unk|>\n"
          ]
        }
      ],
      "source": [
        "# The unknown token now maps back to the `<|unk|>` placeholder during decoding\n",
        "text = tokenizer.decode(ids)\n",
        "print(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "582UB0Se_fza",
      "metadata": {
        "id": "582UB0Se_fza"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
